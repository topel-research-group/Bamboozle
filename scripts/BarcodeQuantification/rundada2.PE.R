#!/usr/bin/env Rscript

# Script generated by adapting commands from nf-core/ampliseq pipeline

library(dada2)
library(digest)
library(dplyr)
library(tidyr)

set.seed(100)

###

# Set variables

path <- "00_data"	# Directory containing (nested) input files
reffile <- "reference.fasta"	# Reference FASTA file, formatted for dada2's assignSpecies

###

outpath <- "01_dada2_out"

if (dir.exists(outpath) == FALSE) {
	dir.create(outpath)
	print("Output directory created.")
}

#list.files(path)

fnFs <- sort(list.files(path, pattern = ".R1.trimmed.fastq.gz", full.names = TRUE, recursive = TRUE))
fnRs <- sort(list.files(path, pattern = ".R2.trimmed.fastq.gz", full.names = TRUE, recursive = TRUE))
sample.names <- sapply(strsplit(basename(fnFs), "[.]"), `[`, 1)

filterpath <- file.path(outpath, "FilteredReads")
filtFs <- file.path(filterpath, paste0(sample.names, ".R1.filt.fastq.gz"))
filtRs <- file.path(filterpath, paste0(sample.names, ".R2.filt.fastq.gz"))

names(filtFs) <- sample.names
names(filtRs) <- sample.names

for (sample in sample.names) {
	infile1 <- paste0(path, "/", sample, "/", sample, ".R1.trimmed.fastq.gz")
	filtfile1 <- filtFs[sample]
	infile2 <- paste0(path, "/", sample, "/", sample, ".R2.trimmed.fastq.gz")
	filtfile2 <- filtRs[sample]

	if (file.exists(filtfile1) & file.exists(filtfile2)) {
		print(paste0(sample, " already filtered."))
	} else {
		print(paste0("Filtering ", sample, "..."))

		out <- filterAndTrim(infile1, filtfile1, infile2, filtfile2, multithread=TRUE, verbose=TRUE)
		out <- cbind(out, ID = row.names(out))

		write.table(out, file = paste0(filterpath, "/", sample,".filter_stats.tsv"), sep = "\t",
			row.names = FALSE, quote = FALSE, na = '')
	}
}

##########################

if (dir.exists(file.path(outpath,"logs")) == FALSE) {
	dir.create(file.path(outpath,"logs"))
	print("Logfile directory created.")
}

if (file.exists(paste0(outpath,"/logs/1_1.err.rds")) & file.exists(paste0(outpath,"/logs/1_2.err.rds"))) {
	print("Errors already learned.")
	errF <- readRDS(paste0(outpath,"/logs/1_1.err.rds"))
	errR <- readRDS(paste0(outpath,"/logs/1_2.err.rds"))
} else {
	print("Learning errors in reads...")
	sink(file = paste0(outpath,"/logs/1.err.log"))
	errF <- learnErrors(filtFs, multithread=TRUE, verbose=TRUE)
	saveRDS(errF, paste0(outpath,"/logs/1_1.err.rds"))
	errR <- learnErrors(filtRs, multithread=TRUE, verbose=TRUE)
	saveRDS(errR, paste0(outpath,"/logs/1_2.err.rds"))
	sink(file = NULL)

	pdf(paste0(outpath,"/logs/1_1.err.pdf"))
	plotErrors(errF, nominalQ = TRUE)
	dev.off()

	pdf(paste0(outpath,"/logs/1_2.err.pdf"))
	plotErrors(errR, nominalQ = TRUE)
	dev.off()

	sink(file = paste0(outpath,"/logs/1_1.err.convergence.txt"))
	dada2:::checkConvergence(errF)
	sink(file = NULL)

	sink(file = paste0(outpath,"/logs/1_2.err.convergence.txt"))
	dada2:::checkConvergence(errR)
	sink(file = NULL)
}

##########################

filtFs <- sort(list.files(filterpath, pattern = ".R1.filt.fastq.gz", full.names = TRUE))
filtRs <- sort(list.files(filterpath, pattern = ".R2.filt.fastq.gz", full.names = TRUE))

if (file.exists(paste0(outpath,"/logs/1_1.dada.rds")) & file.exists(paste0(outpath,"/logs/1_2.dada.rds"))) {
	print("Reads already denoised.")
	dadaFs <- readRDS(paste0(outpath,"/logs/1_1.dada.rds"))
	dadaRs <- readRDS(paste0(outpath,"/logs/1_2.dada.rds"))
} else {
	#denoising
	print("Denoising reads...")
	sink(file = paste0(outpath,"/logs/1.dada.log"))
	dadaFs <- dada(filtFs, err = errF, multithread=TRUE, verbose=TRUE)
	saveRDS(dadaFs, paste0(outpath,"/logs/1_1.dada.rds"))
	dadaRs <- dada(filtRs, err = errR, multithread=TRUE, verbose=TRUE)
	saveRDS(dadaRs, paste0(outpath,"/logs/1_2.dada.rds"))
	sink(file = NULL)
}

##########################

if (file.exists(paste0(outpath,"/logs/1.mergers.rds")) & file.exists(paste0(outpath,"/logs/1.seqtab.rds"))) {
	print("Reads already merged.")
	seqtab <- readRDS(paste0(outpath,"/logs/1.seqtab.rds"))
	mergers <- readRDS(paste0(outpath,"/logs/1.mergers.rds"))
} else {
	print("Merging reads....")
	mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)

	saveRDS(mergers, paste0(outpath,"/logs/1.mergers.rds"))
	seqtab <- makeSequenceTable(mergers)
	saveRDS(seqtab, paste0(outpath,"/logs/1.seqtab.rds"))
}
		
##########################

if (file.exists(paste0(outpath,"/logs/1.ASVtable.rds"))) {
	print("Chimeras already removed.")
	seqtab.nochim <- readRDS(paste0(outpath,"/logs/1.ASVtable.rds"))
} else {
	print("Removing chimeras...")
	seqtab.nochim <- removeBimeraDenovo(seqtab, multithread=TRUE, verbose=TRUE)
	if ( 145 == 1 ) { rownames(seqtab.nochim) <- "P21502_101" }
	saveRDS(seqtab.nochim,paste0(outpath,"/logs/1.ASVtable.rds"))
}

##########################
		
# Combine filter_and_trim files

for (data in list.files(filterpath, pattern = ".filter_stats.tsv", full.names = TRUE)){
	if (!exists("filter_and_trim")){
		filter_and_trim <- read.csv(data, header=TRUE, sep="\t")
	}
	if (exists("filter_and_trim")){
		tempory <-read.csv(data, header=TRUE, sep="\t")
		filter_and_trim <-unique(rbind(filter_and_trim, tempory))
		rm(tempory)
	}
}
rownames(filter_and_trim) <- filter_and_trim$ID
filter_and_trim["ID"] <- NULL
		
# Track reads through pipeline		

getN <- function(x) sum(getUniques(x))
if ( nrow(filter_and_trim) == 1 ) {
	track <- cbind(filter_and_trim, getN(dadaFs), getN(dadaRs), getN(mergers), rowSums(seqtab.nochim))
} else {
	track <- cbind(filter_and_trim, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN),
		rowSums(seqtab.nochim))
}
colnames(track) <- c("DADA2_input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sub(pattern = "_1.fastq.gz$", replacement = "", rownames(track)) #this is when cutadapt is skipped!
track <- cbind(sample = sub(pattern = "(.*?)\\..*$", replacement = "\\1", rownames(track)), track)
write.table( track, file = paste0(outpath,"/1.stats.tsv"), sep = "\t", row.names = FALSE, quote = FALSE, na = '')

##########################

# Combine stats files
for (data in sort(list.files(outpath, pattern = ".stats.tsv", full.names = TRUE))) {
	if (!exists("stats")){
		stats <- read.csv(data, header=TRUE, sep="\t")
	}
	if (exists("stats")){
		temp <-read.csv(data, header=TRUE, sep="\t")
		stats <-unique(rbind(stats, temp))
		rm(temp)
	}
}		

write.table( stats, file = paste0(outpath,"/DADA2_stats.tsv"), sep = "\t", row.names = FALSE, col.names = TRUE,
	quote = FALSE, na = '')

# combine dada-class objects
files <- sort(list.files(paste0(outpath,"/logs"), pattern = ".ASVtable.rds", full.names = TRUE))
if ( length(files) == 1 ) {
	ASVtab <- readRDS(files[1])
} else {
	ASVtab <- mergeSequenceTables(tables=files, repeats = "error", orderBy = "abundance", tryRC = FALSE)
}
saveRDS(ASVtab, paste0(outpath,"/logs/DADA2_table.rds"))

df <- t(ASVtab)
colnames(df) <- gsub('.R1.filt.fastq.gz', '', colnames(df))
colnames(df) <- gsub('.filt.fastq.gz', '', colnames(df))
df <- data.frame(sequence = rownames(df), df, check.names=FALSE)

# Create an md5 sum of the sequences as ASV_ID and rearrange columns

df$sequence <- as.character(df$sequence)
df$ASV_ID <- sapply(df$sequence, digest, algo='md5', serialize = FALSE)
df <- df[,c(ncol(df),3:ncol(df)-1,1)]
	# switches position of first and last column

# file to publish
write.table(df, file = paste0(outpath,"/DADA2_table.tsv"), sep = "\t", row.names = FALSE, quote = FALSE, na = '')

# Write fasta file with ASV sequences to file

write.table(data.frame(s = sprintf(">%s
%s", df$ASV_ID, df$sequence)), paste0(outpath,"/ASV_seqs.fasta"), col.names = FALSE, row.names = FALSE,
	quote = FALSE, na = '')

dfcopy <- df

# Write ASV file with ASV abundances to file
df$sequence <- NULL
write.table(df, file = paste0(outpath,"/ASV_table.tsv"), sep="\t", row.names = FALSE, quote = FALSE, na = '')

##########################

# Output strain-specific data with less parsing in Bash

getstrain <- assignSpecies(dfcopy$sequence, reffile, allowMultiple=TRUE)
getstrain <- as.data.frame(getstrain)
dfcopy$strain <- getstrain$Species

dfcopy$strain <- ifelse(is.na(dfcopy$strain),dfcopy$ASV_ID, dfcopy$strain)

dfcopy <- dfcopy[,c(ncol(dfcopy),3:ncol(dfcopy)-1,1)]

summdf <- as.data.frame(dfcopy %>% mutate(strain = strsplit(as.character(strain), "/")) %>% unnest(strain) %>% group_by(strain) %>% summarise_if(is.numeric, sum))

write.table(summdf, file=paste0(outpath,"/strain_table.tsv"), sep="\t", row.names = FALSE, quote = FALSE, na = '')
saveRDS(summdf, paste0(outpath,"/logs/strain_table.rds"))

##########################

#sessionInfo()
# R version 4.0.2 (2020-06-22)
# Platform: x86_64-apple-darwin17.0 (64-bit)
# Running under: macOS  10.16
# Matrix products: default
# LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
# locale:
# [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
# attached base packages:
# [1] grid      stats     graphics  grDevices utils     datasets  methods   base
# other attached packages:
#  [1] ggbiplot_0.55      devtools_2.4.5     usethis_2.1.6      digest_0.6.31      dada2_1.16.0       Rcpp_1.0.10        gplots_3.1.3       RColorBrewer_1.1-3
#  [9] gridExtra_2.3      ggpubr_0.6.0       broom_1.0.3        ggthemes_4.2.4     cowplot_1.1.1      forcats_1.0.0      stringr_1.5.0      dplyr_1.1.0
# [17] purrr_1.0.1        readr_1.3.1        tidyr_1.3.0        tibble_3.1.8       tidyverse_1.3.0    ggplot2_3.4.1      scales_1.2.1       plyr_1.8.8
# [25] lattice_0.20-45    reshape2_1.4.4
# loaded via a namespace (and not attached):
#   [1] colorspace_2.1-0            ggsignif_0.6.4              deldir_1.0-6                hwriter_1.3.2.1             ellipsis_0.3.2
#   [6] XVector_0.28.0              GenomicRanges_1.40.0        fs_1.6.1                    rstudioapi_0.14             remotes_2.4.2
#  [11] fansi_1.0.4                 lubridate_1.9.2             xml2_1.3.3                  cachem_1.0.6                pkgload_1.3.2
#  [16] jsonlite_1.8.4              Rsamtools_2.4.0             dbplyr_2.3.0                png_0.1-8                   shiny_1.7.4
#  [21] compiler_4.0.2              httr_1.4.4                  backports_1.4.1             assertthat_0.2.1            Matrix_1.2-18
#  [26] fastmap_1.1.0               cli_3.6.0                   later_1.3.0                 prettyunits_1.1.1           htmltools_0.5.4
#  [31] tools_4.0.2                 gtable_0.3.1                glue_1.6.2                  GenomeInfoDbData_1.2.3      tinytex_0.44
#  [36] ShortRead_1.46.0            carData_3.0-5               Biobase_2.48.0              cellranger_1.1.0            vctrs_0.5.2
#  [41] Biostrings_2.56.0           xfun_0.37                   ps_1.7.2                    rvest_1.0.3                 mime_0.12
#  [46] timechange_0.2.0            miniUI_0.1.1.1              lifecycle_1.0.3             gtools_3.9.4                rstatix_0.7.2
#  [51] zlibbioc_1.34.0             promises_1.2.0.1            hms_1.1.2                   parallel_4.0.2              SummarizedExperiment_1.18.2
#  [56] memoise_2.0.1               latticeExtra_0.6-30         stringi_1.7.12              S4Vectors_0.26.1            caTools_1.18.2
#  [61] BiocGenerics_0.34.0         pkgbuild_1.4.0              BiocParallel_1.22.0         GenomeInfoDb_1.24.2         rlang_1.0.6
#  [66] pkgconfig_2.0.3             bitops_1.0-7                matrixStats_0.63.0          htmlwidgets_1.6.1           GenomicAlignments_1.24.0
#  [71] processx_3.8.0              tidyselect_1.2.0            magrittr_2.0.3              R6_2.5.1                    profvis_0.3.7
#  [76] IRanges_2.22.2              generics_0.1.3              DelayedArray_0.14.1         DBI_1.1.3                   pillar_1.8.1
#  [81] haven_2.5.1                 withr_2.5.0                 abind_1.4-5                 RCurl_1.98-1.10             modelr_0.1.10
#  [86] crayon_1.5.2                car_3.1-1                   interp_1.0-33               KernSmooth_2.23-20          utf8_1.2.3
#  [91] urlchecker_1.0.1            jpeg_0.1-8.1                readxl_1.3.1                callr_3.7.3                 reprex_2.0.2
#  [96] xtable_1.8-4                httpuv_1.6.8                RcppParallel_5.1.6          stats4_4.0.2                munsell_0.5.0
# [101] sessioninfo_1.2.2
