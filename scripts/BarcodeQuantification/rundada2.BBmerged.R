#!/usr/bin/env Rscript

# Script generated by adapting commands from nf-core/ampliseq pipeline
# For BBmerged 'single ended' reads

library(dada2)
library(digest)
library(dplyr)
library(tidyr)

set.seed(100)

###

# Set variables

path <- ""	# Directory containing (nested) input files
reffile <- ""	# Reference FASTA file, formatted for dada2's assignSpecies

###

outpath <- "dada2_out"
if (dir.exists(outpath) == FALSE) {
	dir.create(outpath)
	print("Output directory created.")
}

#list.files(path)

fnFs <- sort(list.files(path, pattern = ".merged.fastq.gz", full.names = TRUE, recursive = TRUE))
sample.names <- sapply(strsplit(basename(fnFs), "[.]"), `[`, 1)

filterpath <- file.path(outpath, "FilteredReads")
filtFs <- file.path(filterpath, paste0(sample.names, ".R1.filt.fastq.gz"))

names(filtFs) <- sample.names

for (sample in sample.names) {
	infile1 <- paste0(path, "/", sample, "/", sample, ".merged.fastq.gz")
	filtfile1 <- filtFs[sample]

	if (file.exists(filtfile1)) {
		print(paste0(sample, " already filtered."))
	} else {
		print(paste0("Filtering ", sample, "..."))

		out <- filterAndTrim(infile1, filtfile1, multithread = TRUE, verbose = TRUE)
		out <- cbind(out, ID = row.names(out))

		write.table(out, file = paste0(filterpath, "/", sample,".filter_stats.tsv"), sep = "\t",
			row.names = FALSE, quote = FALSE, na = '')
	}
}

##########################

if (dir.exists(file.path(outpath,"logs")) == FALSE) {
	dir.create(file.path(outpath,"logs"))
	print("Logfile directory created.")
}

if (file.exists(paste0(outpath,"/logs/1_1.err.rds"))) {
	print("Errors already learned.")
	errF <- readRDS(paste0(outpath,"/logs/1_1.err.rds"))
} else {
	print("Learning errors in reads...")
	sink(file = paste0(outpath,"/logs/1.err.log"))

	# Inflate errors threefold to account for pre-merging
	errF <- learnErrors(filtFs, multithread = TRUE, verbose = TRUE)
	errF <- inflateErr(getErrors(errF), 3)
	saveRDS(errF, paste0(outpath,"/logs/1_1.err.rds"))
	sink(file = NULL)
}

##########################

filtFs <- sort(list.files(filterpath, pattern = ".R1.filt.fastq.gz", full.names = TRUE))

if (file.exists(paste0(outpath,"/logs/1_1.dada.rds")) & file.exists(paste0(outpath,"/logs/1.seqtab.rds"))) {
	print("Reads already denoised.")
	dadaFs <- readRDS(paste0(outpath,"/logs/1_1.dada.rds"))
	seqtab <- readRDS(paste0(outpath,"/logs/1.seqtab.rds"))
} else {
	#denoising
	print("Denoising reads...")
	sink(file = paste0(outpath,"/logs/1.dada.log"))

	dadaFs <- dada(filtFs, err = errF, multithread = TRUE)
	saveRDS(dadaFs, paste0(outpath,"/logs/1_1.dada.rds"))
	sink(file = NULL)

	seqtab <- makeSequenceTable(dadaFs)
	saveRDS(seqtab, paste0(outpath,"/logs/1.seqtab.rds"))	
}

##########################

if (file.exists(paste0(outpath,"/logs/1.ASVtable.rds"))) {
	print("Chimeras already removed.")
	seqtab.nochim <- readRDS(paste0(outpath,"/logs/1.ASVtable.rds"))
} else {
	print("Removing chimeras...")

	seqtab.nochim <- removeBimeraDenovo(seqtab, multithread=TRUE, verbose=TRUE)
	if ( 145 == 1 ) { rownames(seqtab.nochim) <- "P21502_101" }
	saveRDS(seqtab.nochim,paste0(outpath,"/logs/1.ASVtable.rds"))
}

##########################
		
# Combine filter_and_trim files

for (data in list.files(filterpath, pattern = ".filter_stats.tsv", full.names = TRUE)){
	if (!exists("filter_and_trim")){
		filter_and_trim <- read.csv(data, header=TRUE, sep="\t")
	}
	if (exists("filter_and_trim")){
		tempory <-read.csv(data, header=TRUE, sep="\t")
		filter_and_trim <-unique(rbind(filter_and_trim, tempory))
		rm(tempory)
	}
}
rownames(filter_and_trim) <- filter_and_trim$ID
filter_and_trim["ID"] <- NULL
		
# Track reads through pipeline		

getN <- function(x) sum(getUniques(x))
if ( nrow(filter_and_trim) == 1 ) {
	track <- cbind(filter_and_trim, getN(dadaFs), rowSums(seqtab.nochim))
} else {
	track <- cbind(filter_and_trim, sapply(dadaFs, getN), rowSums(seqtab.nochim))
}
colnames(track) <- c("DADA2_input", "filtered", "denoisedF", "nonchim")
rownames(track) <- sub(pattern = "_1.fastq.gz$", replacement = "", rownames(track)) #this is when cutadapt is skipped!
track <- cbind(sample = sub(pattern = "(.*?)\\..*$", replacement = "\\1", rownames(track)), track)
write.table( track, file = paste0(outpath,"/1.stats.tsv"), sep = "\t", row.names = FALSE, quote = FALSE, na = '')

##########################

# Combine stats files
for (data in sort(list.files(outpath, pattern = ".stats.tsv", full.names = TRUE))) {
	if (!exists("stats")){
		stats <- read.csv(data, header=TRUE, sep="\t")
	}
	if (exists("stats")){
		temp <-read.csv(data, header=TRUE, sep="\t")
		stats <-unique(rbind(stats, temp))
		rm(temp)
	}
}		

write.table( stats, file = paste0(outpath,"/DADA2_stats.tsv"), sep = "\t", row.names = FALSE, col.names = TRUE,
	quote = FALSE, na = '')

# combine dada-class objects
files <- sort(list.files(paste0(outpath,"/logs"), pattern = ".ASVtable.rds", full.names = TRUE))
if ( length(files) == 1 ) {
	ASVtab <- readRDS(files[1])
} else {
	ASVtab <- mergeSequenceTables(tables=files, repeats = "error", orderBy = "abundance", tryRC = FALSE)
}
saveRDS(ASVtab, paste0(outpath,"/logs/DADA2_table.rds"))

df <- t(ASVtab)
colnames(df) <- gsub('.R1.filt.fastq.gz', '', colnames(df))
colnames(df) <- gsub('.filt.fastq.gz', '', colnames(df))
df <- data.frame(sequence = rownames(df), df, check.names=FALSE)

# Create an md5 sum of the sequences as ASV_ID and rearrange columns

df$sequence <- as.character(df$sequence)
df$ASV_ID <- sapply(df$sequence, digest, algo='md5', serialize = FALSE)
df <- df[,c(ncol(df),3:ncol(df)-1,1)]
	# switches position of first and last column

# file to publish
write.table(df, file = paste0(outpath,"/DADA2_table.tsv"), sep = "\t", row.names = FALSE, quote = FALSE, na = '')

# Write fasta file with ASV sequences to file

write.table(data.frame(s = sprintf(">%s
%s", df$ASV_ID, df$sequence)), paste0(outpath,"/ASV_seqs.fasta"), col.names = FALSE, row.names = FALSE,
	quote = FALSE, na = '')

dfcopy <- df

# Write ASV file with ASV abundances to file
df$sequence <- NULL
write.table(df, file = paste0(outpath,"/ASV_table.tsv"), sep="\t", row.names = FALSE, quote = FALSE, na = '')

##########################

# Output strain-specific data with less parsing in Bash

getstrain <- assignSpecies(dfcopy$sequence, reffile, allowMultiple=TRUE)
getstrain <- as.data.frame(getstrain)
dfcopy$strain <- getstrain$Species

dfcopy$strain <- ifelse(is.na(dfcopy$strain),dfcopy$ASV_ID, dfcopy$strain)

dfcopy <- dfcopy[,c(ncol(dfcopy),3:ncol(dfcopy)-1,1)]

summdf <- as.data.frame(dfcopy %>% mutate(strain = strsplit(as.character(strain), "/")) %>% unnest(strain) %>% group_by(strain) %>% summarise_if(is.numeric, sum))

write.table(summdf, file=paste0(outpath,"/strain_table.tsv"), sep="\t", row.names = FALSE, quote = FALSE, na = '')
saveRDS(summdf, paste0(outpath,"/logs/strain_table.rds"))

